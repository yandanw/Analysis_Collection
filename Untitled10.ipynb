{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/yandanw/.local/lib/python3.6/site-packages/ants/viz/render_surface_function.py:16: UserWarning:\n",
      "\n",
      "Cant import Plotly. Install it `pip install chart_studio` if you want to use ants.render_surface_function\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "import scipy\n",
    "import nibabel as nib\n",
    "import bigbadbrain as bbb\n",
    "import dataflow as flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "### Load Superslice ###\n",
    "#######################\n",
    "### A superslice is a single z-plane but all flies have already been concatenated along an axis of this array\n",
    "brain_file = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20201129_super_slices/superslice_{}.nii\".format(20)\n",
    "brain = np.array(nib.load(brain_file).get_data(), copy=True)\n",
    "# # Delete a fly that is in the superslice but was excluded from all analysis due to not passing quality control\n",
    "# fly_idx_delete = 3 #(fly_095)\n",
    "# brain = np.delete(brain, fly_idx_delete, axis=-1) #### DELETING FLY_095 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 128, 3384, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### Load Superslice ###\n",
    "################################\n",
    "### from 0307Jupyternotebook ###\n",
    "################################\n",
    "data_dir = '/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset'\n",
    "superfly_path = os.path.join(data_dir, '20240327_superfly', 'superslices')\n",
    "#########################\n",
    "### POST-WARP LOADING ###\n",
    "#########################\n",
    "\n",
    "n_clusters = 2000\n",
    "\n",
    "load_file = os.path.join(superfly_path, 'cluster_labels.npy')\n",
    "cluster_labels = np.load(load_file)\n",
    "\n",
    "load_file = os.path.join(superfly_path, 'cluster_signals.npy')\n",
    "all_signals = np.load(load_file)\n",
    "\n",
    "dim_z = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 2000, 10152)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(all_signals )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from Bella GitHub\n",
    "# class Fly:\n",
    "#     def __init__ (self, fly_name, fly_idx):\n",
    "#         self.dir = os.path.join(dataset_path, fly_name, 'func_0')\n",
    "#         self.fly_idx = fly_idx\n",
    "#         self.fly_name = fly_name\n",
    "#         self.maps = {}\n",
    "#     def load_timestamps (self):\n",
    "#         self.timestamps = bbb.load_timestamps(os.path.join(self.dir, 'imaging'))\n",
    "#     def load_fictrac (self):\n",
    "#         self.fictrac = Fictrac(self.dir, self.timestamps)\n",
    "#     def load_brain_slice (self):\n",
    "#         self.brain = brain[:,:,:,self.fly_idx]\n",
    "#     def load_anatomy (self):\n",
    "#         to_load = os.path.join(dataset_path, self.fly_name, 'warp', 'anat-to-meanbrain.nii')\n",
    "#         self.anatomy = np.array(nib.load(to_load).get_data(), copy=True)\n",
    "#     def load_z_depth_correction (self):\n",
    "#         to_load = os.path.join(dataset_path, self.fly_name, 'warp', '20201220_warped_z_depth.nii')\n",
    "#         self.z_correction = np.array(nib.load(to_load).get_data(), copy=True)\n",
    "#     def get_cluster_averages (self, cluster_model_labels, n_clusters):\n",
    "#         neural_data = self.brain.reshape(-1, 3384)\n",
    "#         signals = []\n",
    "#         self.cluster_indicies = []\n",
    "#         for cluster_num in range(n_clusters):\n",
    "#             cluster_indicies = np.where(cluster_model_labels==cluster_num)[0]\n",
    "#             mean_signal = np.mean(neural_data[cluster_indicies,:], axis=0)\n",
    "#             signals.append(mean_signal)\n",
    "#             self.cluster_indicies.append(cluster_indicies) # store for later\n",
    "#         self.cluster_signals=np.asarray(signals)\n",
    "#     def get_cluster_id (self, x, y):\n",
    "#         ax_vec = x*128 + y\n",
    "#         for i in range(n_clusters):\n",
    "#             if ax_vec in self.cluster_indicies[i]:\n",
    "#                 cluster_id = i\n",
    "#                 break\n",
    "#         return cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Fly:\n",
    "    def __init__(self, fly_name, fly_idx, data_dir, all_signals):\n",
    "        self.dir = os.path.join(data_dir, fly_name, 'func_0')\n",
    "        self.fly_idx = fly_idx\n",
    "        self.fly_name = fly_name\n",
    "        self.maps = {}\n",
    "        self.all_signals = all_signals\n",
    "\n",
    "    def load_timestamps(self):\n",
    "        self.timestamps = bbb.load_timestamps(os.path.join(self.dir, 'imaging'))\n",
    "\n",
    "    def load_fictrac(self):\n",
    "        self.fictrac = Fictrac(self.dir, self.timestamps)\n",
    "\n",
    "    def load_brain_slice(self):\n",
    "        self.brain = self.all_signals[:,:,:,self.fly_idx]\n",
    "\n",
    "    def get_cluster_averages(self, cluster_model_labels, n_clusters):\n",
    "        neural_data = self.brain.reshape(-1, self.brain.shape[-1])\n",
    "        signals = []\n",
    "        self.cluster_indicies = []\n",
    "        for cluster_num in range(n_clusters):\n",
    "            cluster_indicies = np.where(cluster_model_labels == cluster_num)[0]\n",
    "            mean_signal = np.mean(neural_data[cluster_indicies, :], axis=0)\n",
    "            signals.append(mean_signal)\n",
    "            self.cluster_indicies.append(cluster_indicies)\n",
    "        self.cluster_signals = np.asarray(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FROM BELLA GITHUB\n",
    "class Fictrac:\n",
    "    def __init__ (self, fly_dir, timestamps):\n",
    "        self.fictrac_raw = bbb.load_fictrac(os.path.join(fly_dir, 'fictrac'))\n",
    "        self.timestamps = timestamps\n",
    "    def make_interp_object(self, behavior):\n",
    "        # Create camera timepoints\n",
    "        fps=100 #50\n",
    "        camera_rate = 1/fps * 1000 # camera frame rate in ms\n",
    "        expt_len = 1000*30*60\n",
    "        x_original = np.arange(0,expt_len,camera_rate)\n",
    "\n",
    "        # Smooth raw fictrac data\n",
    "        fictrac_smoothed = scipy.signal.savgol_filter(np.asarray(self.fictrac_raw[behavior]),25,3)\n",
    "\n",
    "        # Create interp object with camera timepoints\n",
    "        fictrac_interp_object = interp1d(x_original, fictrac_smoothed, bounds_error = False)\n",
    "        return fictrac_interp_object\n",
    "\n",
    "    def pull_from_interp_object(self, interp_object, timepoints):\n",
    "        new_interp = interp_object(timepoints)\n",
    "        np.nan_to_num(new_interp, copy=False);\n",
    "        return new_interp\n",
    "\n",
    "    def interp_fictrac(self, z):\n",
    "        behaviors = ['dRotLabY', 'dRotLabZ']; shorts = ['Y', 'Z']\n",
    "        self.fictrac = {}\n",
    "\n",
    "        for behavior, short in zip(behaviors, shorts):\n",
    "            interp_object = self.make_interp_object(behavior)\n",
    "            self.fictrac[short + 'i'] = interp_object\n",
    "\n",
    "            ### Velocity ###\n",
    "            low_res_behavior = self.pull_from_interp_object(interp_object, self.timestamps[:,z])\n",
    "            self.fictrac[short] = low_res_behavior#/np.std(low_res_behavior)\n",
    "\n",
    "            ### Clipped Velocities ###\n",
    "            self.fictrac[short + '_pos'] = np.clip(self.fictrac[short], a_min=0, a_max=None)\n",
    "            self.fictrac[short + '_neg'] = np.clip(self.fictrac[short], a_min=None, a_max=0)*-1\n",
    "\n",
    "            ### Strongly Clipped Velocities ###\n",
    "            # excludes points even close to 0\n",
    "            #self.fictrac[short + '_pos_very'] = np.clip(self.fictrac[short], a_min=0.3, a_max=None)\n",
    "            #self.fictrac[short + '_neg_very'] = np.clip(self.fictrac[short], a_min=None, a_max=-0.3)*-1\n",
    "\n",
    "            ### Acceleration ###\n",
    "            high_res_behavior = self.pull_from_interp_object(interp_object, high_res_timepoints)\n",
    "            self.fictrac[short + 'h'] = high_res_behavior/np.std(high_res_behavior)\n",
    "\n",
    "            accel = scipy.signal.savgol_filter(np.diff(high_res_behavior),25,3)\n",
    "            accel = np.append(accel, 0)\n",
    "            interp_object = interp1d(high_res_timepoints, accel, bounds_error = False)\n",
    "            acl = interp_object(self.timestamps[:,z])\n",
    "            acl[-1] = 0\n",
    "            self.fictrac[short + 'a'] = acl#/np.std(acl)\n",
    "\n",
    "            ### Clipped Acceleration ###\n",
    "            self.fictrac[short + 'a' + '_pos'] = np.clip(self.fictrac[short + 'a'], a_min=0, a_max=None)\n",
    "            self.fictrac[short + 'a' + '_neg'] = np.clip(self.fictrac[short + 'a'], a_min=None, a_max=0)*-1\n",
    "\n",
    "        self.fictrac['YZ'] = np.sqrt(np.power(self.fictrac['Y'],2), np.power(self.fictrac['Z'],2))\n",
    "        self.fictrac['YZh'] = np.sqrt(np.power(self.fictrac['Yh'],2), np.power(self.fictrac['Zh'],2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Fly:\n",
    "    def __init__(self, fly_name, fly_index, data_dir, all_signals, num_timepoints_per_fly):\n",
    "        self.dir = os.path.join(data_dir, fly_name, 'func_0')\n",
    "        self.fly_name = fly_name\n",
    "        self.maps = {}\n",
    "        # Calculate start and end indices for slicing all_signals array\n",
    "        start_idx = fly_index * num_timepoints_per_fly\n",
    "        end_idx = start_idx + num_timepoints_per_fly\n",
    "        # Extract signals for this particular fly\n",
    "        self.brain_signals = all_signals[:, :, start_idx:end_idx]\n",
    "\n",
    "    def load_timestamps(self):\n",
    "        self.timestamps = bbb.load_timestamps(os.path.join(self.dir, 'imaging'))\n",
    "\n",
    "    def load_fictrac(self):\n",
    "        self.fictrac = Fictrac(self.dir, self.timestamps)\n",
    "\n",
    "    def get_cluster_averages(self, cluster_model_labels):\n",
    "        # This method now assumes that the clustering is already handled in all_signals\n",
    "        self.cluster_signals = self.all_signals  # Directly use all_signals assuming it's already properly formatted\n",
    "\n",
    "class Fictrac:\n",
    "    def __init__(self, fly_dir, timestamps):\n",
    "        self.dir = fly_dir\n",
    "        self.timestamps = timestamps\n",
    "        self.fictrac_raw = bbb.load_fictrac(os.path.join(fly_dir, 'fictrac'))\n",
    "        self.fictrac = {}  # Initialize the fictrac dictionary to store processed data\n",
    "\n",
    "    def make_interp_object(self, behavior):\n",
    "        fps = 100\n",
    "        camera_rate = 1 / fps * 1000  # ms per frame\n",
    "        expt_len = 1000*30*60  # Adjusted to match timestamps\n",
    "        x_original = np.arange(0, expt_len, camera_rate)\n",
    "        fictrac_smoothed = scipy.signal.savgol_filter(np.asarray(self.fictrac_raw[behavior]),25,3)\n",
    "        # Create interp object with camera timepoints\n",
    "        fictrac_interp_object = interp1d(x_original, fictrac_smoothed, bounds_error = False)\n",
    "        return fictrac_interp_object\n",
    "\n",
    "    def pull_from_interp_object(self, interp_object, timepoints):\n",
    "        new_interp = interp_object(timepoints)\n",
    "        return np.nan_to_num(new_interp, copy=False)\n",
    "\n",
    "    def interp_fictrac(self):\n",
    "        behaviors = ['dRotLabY', 'dRotLabZ']\n",
    "        shorts = ['Y', 'Z']\n",
    "        self.fictrac = {}\n",
    "        for behavior, short in zip(behaviors, shorts):\n",
    "            interp_object = self.make_interp_object(behavior)\n",
    "            self.fictrac[short + 'i'] = interp_object\n",
    "            low_res_behavior = self.pull_from_interp_object(interp_object, self.timestamps)\n",
    "            self.fictrac[short] = low_res_behavior  # Normalized/processed further if needed\n",
    "            \n",
    "            ### Clipped Velocities ###\n",
    "            self.fictrac[short + '_pos'] = np.clip(self.fictrac[short], a_min=0, a_max=None)\n",
    "            self.fictrac[short + '_neg'] = np.clip(self.fictrac[short], a_min=None, a_max=0)*-1\n",
    "            \n",
    "            ### Acceleration ###\n",
    "            high_res_behavior = self.pull_from_interp_object(interp_object, high_res_timepoints)\n",
    "            self.fictrac[short + 'h'] = high_res_behavior/np.std(high_res_behavior)\n",
    "            accel = scipy.signal.savgol_filter(np.diff(high_res_behavior),25,3)\n",
    "            accel = np.append(accel, 0)\n",
    "            interp_object = interp1d(high_res_timepoints, accel, bounds_error = False)\n",
    "            acl = interp_object(self.timestamps[:,z])\n",
    "            acl[-1] = 0\n",
    "            self.fictrac[short + 'a'] = acl#/np.std(acl)\n",
    "            \n",
    "            ### Clipped Acceleration ###\n",
    "            self.fictrac[short + 'a' + '_pos'] = np.clip(self.fictrac[short + 'a'], a_min=0, a_max=None)\n",
    "            self.fictrac[short + 'a' + '_neg'] = np.clip(self.fictrac[short + 'a'], a_min=None, a_max=0)*-1\n",
    "        self.fictrac['YZ'] = np.sqrt(np.power(self.fictrac['Y'],2), np.power(self.fictrac['Z'],2))\n",
    "        self.fictrac['YZh'] = np.sqrt(np.power(self.fictrac['Yh'],2), np.power(self.fictrac['Zh'],2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset\"\n",
    "fly_names = ['fly_134', 'fly_292', 'fly_294']\n",
    "cluster_labels = np.load(os.path.join(data_dir, '20240327_superfly', 'superslices', 'cluster_labels.npy'))\n",
    "all_signals = np.load(os.path.join(data_dir, '20240327_superfly', 'superslices', 'cluster_signals.npy'))\n",
    "# cluster_labels = np.load(\"/path/to/cluster_labels.npy\")\n",
    "# all_signals = np.load(\"/path/to/all_signals.npy\")\n",
    "num_timepoints_per_fly = 3384\n",
    "behavior_to_corr = 'Y'  # Define which behavior to correlate\n",
    "n_clusters = 2000\n",
    "z = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 5.30 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 8.72 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 4.36 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 8.12 sec\n",
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 4.79 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 7.91 sec\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Fly' object has no attribute 'cluster_signals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3c62929b5cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# n_clusters = 2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# z = 20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mr_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfly_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_signals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_timepoints_per_fly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior_to_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-3c62929b5cb8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_dir, fly_names, all_signals, cluster_labels, num_timepoints_per_fly, behavior_to_corr, n_clusters)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpooled_activity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfly\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mpooled_activity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mpooled_activity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_activity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Fly' object has no attribute 'cluster_signals'"
     ]
    }
   ],
   "source": [
    "def main(data_dir, fly_names, all_signals, cluster_labels, num_timepoints_per_fly, behavior_to_corr, n_clusters):\n",
    "    flies = {}\n",
    "    for i, fly_name in enumerate(fly_names):\n",
    "        flies[fly_name] = Fly(fly_name, i, data_dir, all_signals, num_timepoints_per_fly)\n",
    "        flies[fly_name].load_timestamps()\n",
    "        flies[fly_name].load_fictrac()\n",
    "        flies[fly_name].fictrac.interp_fictrac()  # Ensure this is called to populate fictrac dictionary\n",
    "\n",
    "    # Pool behavior\n",
    "    not_clipped_behaviors = ['Y', 'Z', 'Ya', 'Za']\n",
    "    clipped_behaviors = ['Y_pos', 'Y_neg', 'Z_pos', 'Z_neg', 'Ya_pos', 'Ya_neg', 'Za_pos', 'Za_neg']\n",
    "    all_behaviors = not_clipped_behaviors + clipped_behaviors\n",
    "\n",
    "    pooled_behavior = {}\n",
    "    for j, behavior in enumerate(all_behaviors):\n",
    "        pooled_behavior[behavior] = []\n",
    "        for fly in flies.values():\n",
    "            pooled_behavior[behavior].append(fly.fictrac.fictrac[behavior])\n",
    "        pooled_behavior[behavior] = np.asarray(pooled_behavior[behavior]).flatten()\n",
    "\n",
    "    # Correlation\n",
    "    r_values = []\n",
    "    p_values = []\n",
    "    for cluster in range(n_clusters):\n",
    "        pooled_activity = []\n",
    "        for fly in flies.values():\n",
    "            pooled_activity.append(fly.cluster_signals[cluster])\n",
    "        pooled_activity = np.asarray(pooled_activity).flatten()\n",
    "\n",
    "        Y = pooled_activity\n",
    "        X = pooled_behavior[behavior_to_corr]\n",
    "\n",
    "        r, p = scipy.stats.pearsonr(X, Y) # calculate correlation\n",
    "        r_values.append(r)\n",
    "        p_values.append(p)\n",
    "\n",
    "    # Output results or return them\n",
    "    return r_values, p_values\n",
    "\n",
    "# Example call to main with updated parameters\n",
    "# data_dir = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset\"\n",
    "# fly_names = ['fly_134', 'fly_292', 'fly_294']\n",
    "# cluster_labels = np.load(os.path.join(data_dir, '20240327_superfly', 'superslices', 'cluster_labels.npy'))\n",
    "# all_signals = np.load(os.path.join(data_dir, '20240327_superfly', 'superslices', 'cluster_signals.npy'))\n",
    "# # cluster_labels = np.load(\"/path/to/cluster_labels.npy\")\n",
    "# # all_signals = np.load(\"/path/to/all_signals.npy\")\n",
    "# num_timepoints_per_fly = 3384\n",
    "# behavior_to_corr = 'Y'  # Define which behavior to correlate\n",
    "# n_clusters = 2000\n",
    "# z = 20\n",
    "r_values, p_values = main(data_dir, fly_names, all_signals, cluster_labels, num_timepoints_per_fly, behavior_to_corr, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset\"\n",
    "fly_names = ['fly_134', 'fly_292', 'fly_294']\n",
    "expt_len = 1000*30*60\n",
    "resolution = 10\n",
    "high_res_timepoints = np.arange(0,expt_len,resolution) #0 to last time at subsample res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~ load_timestamps ~~\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n",
      "load_timestamps done. Duration: 72.49 ms\n",
      "\n",
      "~~ load_fictrac ~~\n",
      "load_fictrac done. Duration: 8.27 sec\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 3-dimensional, but 4 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1399ef681ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_signals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'20240327_superfly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'superslices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cluster_signals.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfly_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_signals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-af9f3b2c7ee7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_dir, fly_names, all_signals, cluster_labels, n_clusters)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mflies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfly_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_timestamps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mflies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfly_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_fictrac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mflies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfly_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_brain_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mflies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfly_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cluster_averages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-811de7f4828f>\u001b[0m in \u001b[0;36mload_brain_slice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_brain_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfly_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_cluster_averages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_model_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 3-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "data_dir = \"/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset\"\n",
    "fly_names = ['fly_134', 'fly_292', 'fly_294']\n",
    "cluster_labels = np.load(os.path.join(data_dir, '20240327_superfly', 'superslices', 'cluster_labels.npy'))\n",
    "all_signals = np.load(os.path.join(data_dir, '20240327_superfly', 'superslices', 'cluster_signals.npy'))\n",
    "n_clusters = 2000\n",
    "main(data_dir, fly_names, all_signals, cluster_labels, n_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
